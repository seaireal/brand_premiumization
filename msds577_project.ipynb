{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import itertools\n",
    "import re\n",
    "import collections\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>product</th>\n",
       "      <th>description</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>brand</th>\n",
       "      <th>company</th>\n",
       "      <th>company_parent</th>\n",
       "      <th>market_country</th>\n",
       "      <th>store_name</th>\n",
       "      <th>store_type</th>\n",
       "      <th>import_status</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>percentile</th>\n",
       "      <th>premium</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>description_token</th>\n",
       "      <th>description_wv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7146579</td>\n",
       "      <td>Nocturnal Nectar Bio-Fermented Facial Essence</td>\n",
       "      <td>Named after an English wild-flower meadow, Aml...</td>\n",
       "      <td>Face/Neck Care</td>\n",
       "      <td>Amly</td>\n",
       "      <td>Amly Botanicals</td>\n",
       "      <td>Amly Botanicals</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Imported</td>\n",
       "      <td>402266.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.33388913, 2.1469963, 0.96274215, -0.878904...</td>\n",
       "      <td>[name, english, wild, flower, meadow, amli, de...</td>\n",
       "      <td>[[-0.051982604, -0.08290577, 0.3159616, 0.0085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6444419</td>\n",
       "      <td>Micro Needling Pimple Patches</td>\n",
       "      <td>Vice Reversa aims to focus on targeted deliver...</td>\n",
       "      <td>Face/Neck Care</td>\n",
       "      <td>Vice Reversa</td>\n",
       "      <td>A Beauty Story</td>\n",
       "      <td>A Beauty Story</td>\n",
       "      <td>UK</td>\n",
       "      <td>Victoria Health</td>\n",
       "      <td>Internet/Mail Order</td>\n",
       "      <td>Imported</td>\n",
       "      <td>265200.00</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.59363365, 2.8043451, 0.6298298, -1.0089695,...</td>\n",
       "      <td>[vice, reversa, aim, focu, target, deliveri, s...</td>\n",
       "      <td>[[0.3694772, -0.57862556, -2.0269456, -0.64512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6993711</td>\n",
       "      <td>Capsules Protection Totale Face Treatment Oil</td>\n",
       "      <td>The Jean d'Arcel Renovar collection is said to...</td>\n",
       "      <td>Face/Neck Care</td>\n",
       "      <td>Jean d'Arcel Renovar</td>\n",
       "      <td>Jean d'Arcel Cosmétique</td>\n",
       "      <td>Jean d'Arcel Cosmétique</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Feel-Beauty.de</td>\n",
       "      <td>Internet/Mail Order</td>\n",
       "      <td>Not imported</td>\n",
       "      <td>242950.00</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.39346665, -1.5720978, 0.32630467, -0.300545...</td>\n",
       "      <td>[jean, arcel, renovar, collect, said, featur, ...</td>\n",
       "      <td>[[0.8480468, -0.055171926, -0.54320484, 0.1345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6444421</td>\n",
       "      <td>Micro Needling Plumping Patches</td>\n",
       "      <td>Vice Reversa aims to focus on targeted deliver...</td>\n",
       "      <td>Face/Neck Care</td>\n",
       "      <td>Vice Reversa</td>\n",
       "      <td>A Beauty Story</td>\n",
       "      <td>A Beauty Story</td>\n",
       "      <td>UK</td>\n",
       "      <td>Victoria Health</td>\n",
       "      <td>Internet/Mail Order</td>\n",
       "      <td>Imported</td>\n",
       "      <td>232350.00</td>\n",
       "      <td>0.999817</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.16396545, 2.3078039, 1.7311268, -0.7228534,...</td>\n",
       "      <td>[vice, reversa, aim, focu, target, deliveri, s...</td>\n",
       "      <td>[[0.3694772, -0.57862556, -2.0269456, -0.64512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6511823</td>\n",
       "      <td>Dermatite, Psoriasis, Eczema CBD (5%) Skin Rep...</td>\n",
       "      <td>You &amp; Oil products for skin and hair are creat...</td>\n",
       "      <td>Body Care</td>\n",
       "      <td>You &amp; Oil Phyto Derma Therapy</td>\n",
       "      <td>JSC Biokosmetikos</td>\n",
       "      <td>JSC Biokosmetikos</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Savue</td>\n",
       "      <td>Internet/Mail Order</td>\n",
       "      <td>nan</td>\n",
       "      <td>63360.00</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.53203, 1.2907141, -0.31864712, -1.9782542, ...</td>\n",
       "      <td>[oil, product, skin, hair, creat, exclus, ecol...</td>\n",
       "      <td>[[0.9449418, -0.19407645, 0.33627698, -1.56030...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3287</td>\n",
       "      <td>7111519</td>\n",
       "      <td>Body Lotion</td>\n",
       "      <td>Edeka Elkos Body Body Lotion has been relaunch...</td>\n",
       "      <td>Body Care</td>\n",
       "      <td>Edeka Elkos Body</td>\n",
       "      <td>Edeka Zentrale</td>\n",
       "      <td>Edeka Zentrale</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Diska</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.3947046, 0.4628543, -0.00500729, 0.6803158...</td>\n",
       "      <td>[edeka, elko, bodi, bodi, lotion, relaunch, ne...</td>\n",
       "      <td>[[0.16146915, 0.81466347, 0.26688766, 1.377136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3288</td>\n",
       "      <td>6703603</td>\n",
       "      <td>Special Natural Clay Powder</td>\n",
       "      <td>Seara Argila Natural Especial em Pó (Special N...</td>\n",
       "      <td>Body Care</td>\n",
       "      <td>Seara</td>\n",
       "      <td>Seara Produtos Naturais</td>\n",
       "      <td>Seara Produtos Naturais</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Continente</td>\n",
       "      <td>Mass Merchandise/Hypermarket</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0026945, -0.29684862, 0.1989893, 0.28999498...</td>\n",
       "      <td>[seara, argila, natur, especi, em, pó, special...</td>\n",
       "      <td>[[0.24507636, 0.19971885, 0.1813712, 0.0929433...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3289</td>\n",
       "      <td>6331657</td>\n",
       "      <td>Coenzyme Q10 Body Milk</td>\n",
       "      <td>Amalfi Skin Care Leite Corporal Coenzima Q10 (...</td>\n",
       "      <td>Body Care</td>\n",
       "      <td>Amalfi Skin Care</td>\n",
       "      <td>Quimi Romar</td>\n",
       "      <td>Quimi Romar</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Jumbo Auchan</td>\n",
       "      <td>Mass Merchandise/Hypermarket</td>\n",
       "      <td>Imported</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.060306523, 0.06860288, 0.31759515, 0.22624...</td>\n",
       "      <td>[amalfi, skin, care, leit, corpor, coenzima, c...</td>\n",
       "      <td>[[-1.9248099, 0.80903107, 0.48631546, 0.015191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>6855981</td>\n",
       "      <td>Body Milk</td>\n",
       "      <td>Amalfi Skin Care Rosa Mosqueta Leche Corporal ...</td>\n",
       "      <td>Body Care</td>\n",
       "      <td>Amalfi Skin Care Rosa Mosqueta</td>\n",
       "      <td>Quimi Romar</td>\n",
       "      <td>Quimi Romar</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Carrefour</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>Not imported</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.009814276, 0.013060865, 0.09264118, 0.71502...</td>\n",
       "      <td>[amalfi, skin, care, rosa, mosqueta, lech, cor...</td>\n",
       "      <td>[[-1.9248099, 0.80903107, 0.48631546, 0.015191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3291</td>\n",
       "      <td>6600153</td>\n",
       "      <td>Rose Water</td>\n",
       "      <td>Sandy Gül Suyu (Rose Water) can be used as a t...</td>\n",
       "      <td>Face/Neck Care</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>Pereja Ileri Kimya San</td>\n",
       "      <td>Pereja Ileri Kimya San</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>A.101</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>Not imported</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.50167626, 0.11863858, 0.26922804, -0.39989...</td>\n",
       "      <td>[sandi, gül, suyu, rose, water, use, tonic, sk...</td>\n",
       "      <td>[[-0.1723986, 0.033396434, 0.74005365, -0.0430...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3292 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id                                            product  \\\n",
       "0       7146579      Nocturnal Nectar Bio-Fermented Facial Essence   \n",
       "1       6444419                      Micro Needling Pimple Patches   \n",
       "2       6993711      Capsules Protection Totale Face Treatment Oil   \n",
       "3       6444421                    Micro Needling Plumping Patches   \n",
       "4       6511823  Dermatite, Psoriasis, Eczema CBD (5%) Skin Rep...   \n",
       "...         ...                                                ...   \n",
       "3287    7111519                                        Body Lotion   \n",
       "3288    6703603                        Special Natural Clay Powder   \n",
       "3289    6331657                             Coenzyme Q10 Body Milk   \n",
       "3290    6855981                                          Body Milk   \n",
       "3291    6600153                                         Rose Water   \n",
       "\n",
       "                                            description    sub_category  \\\n",
       "0     Named after an English wild-flower meadow, Aml...  Face/Neck Care   \n",
       "1     Vice Reversa aims to focus on targeted deliver...  Face/Neck Care   \n",
       "2     The Jean d'Arcel Renovar collection is said to...  Face/Neck Care   \n",
       "3     Vice Reversa aims to focus on targeted deliver...  Face/Neck Care   \n",
       "4     You & Oil products for skin and hair are creat...       Body Care   \n",
       "...                                                 ...             ...   \n",
       "3287  Edeka Elkos Body Body Lotion has been relaunch...       Body Care   \n",
       "3288  Seara Argila Natural Especial em Pó (Special N...       Body Care   \n",
       "3289  Amalfi Skin Care Leite Corporal Coenzima Q10 (...       Body Care   \n",
       "3290  Amalfi Skin Care Rosa Mosqueta Leche Corporal ...       Body Care   \n",
       "3291  Sandy Gül Suyu (Rose Water) can be used as a t...  Face/Neck Care   \n",
       "\n",
       "                               brand                  company  \\\n",
       "0                               Amly          Amly Botanicals   \n",
       "1                       Vice Reversa           A Beauty Story   \n",
       "2               Jean d'Arcel Renovar  Jean d'Arcel Cosmétique   \n",
       "3                       Vice Reversa           A Beauty Story   \n",
       "4      You & Oil Phyto Derma Therapy        JSC Biokosmetikos   \n",
       "...                              ...                      ...   \n",
       "3287                Edeka Elkos Body           Edeka Zentrale   \n",
       "3288                           Seara  Seara Produtos Naturais   \n",
       "3289                Amalfi Skin Care              Quimi Romar   \n",
       "3290  Amalfi Skin Care Rosa Mosqueta              Quimi Romar   \n",
       "3291                           Sandy   Pereja Ileri Kimya San   \n",
       "\n",
       "               company_parent market_country       store_name  \\\n",
       "0             Amly Botanicals        Germany             None   \n",
       "1              A Beauty Story             UK  Victoria Health   \n",
       "2     Jean d'Arcel Cosmétique        Germany   Feel-Beauty.de   \n",
       "3              A Beauty Story             UK  Victoria Health   \n",
       "4           JSC Biokosmetikos        Germany            Savue   \n",
       "...                       ...            ...              ...   \n",
       "3287           Edeka Zentrale        Germany            Diska   \n",
       "3288  Seara Produtos Naturais       Portugal       Continente   \n",
       "3289              Quimi Romar       Portugal     Jumbo Auchan   \n",
       "3290              Quimi Romar          Spain        Carrefour   \n",
       "3291   Pereja Ileri Kimya San         Turkey            A.101   \n",
       "\n",
       "                        store_type import_status  unit_price  percentile  \\\n",
       "0                             None      Imported   402266.00    1.000000   \n",
       "1              Internet/Mail Order      Imported   265200.00    0.999939   \n",
       "2              Internet/Mail Order  Not imported   242950.00    0.999878   \n",
       "3              Internet/Mail Order      Imported   232350.00    0.999817   \n",
       "4              Internet/Mail Order           nan    63360.00    0.999757   \n",
       "...                            ...           ...         ...         ...   \n",
       "3287                   Supermarket           nan        0.25    0.000122   \n",
       "3288  Mass Merchandise/Hypermarket           nan        0.25    0.000122   \n",
       "3289  Mass Merchandise/Hypermarket      Imported        0.25    0.000122   \n",
       "3290                   Supermarket  Not imported        0.23    0.000061   \n",
       "3291                   Supermarket  Not imported        0.18    0.000000   \n",
       "\n",
       "      premium                                            doc_vec  \\\n",
       "0           1  [-0.33388913, 2.1469963, 0.96274215, -0.878904...   \n",
       "1           1  [0.59363365, 2.8043451, 0.6298298, -1.0089695,...   \n",
       "2           1  [0.39346665, -1.5720978, 0.32630467, -0.300545...   \n",
       "3           1  [0.16396545, 2.3078039, 1.7311268, -0.7228534,...   \n",
       "4           1  [1.53203, 1.2907141, -0.31864712, -1.9782542, ...   \n",
       "...       ...                                                ...   \n",
       "3287        0  [-1.3947046, 0.4628543, -0.00500729, 0.6803158...   \n",
       "3288        0  [1.0026945, -0.29684862, 0.1989893, 0.28999498...   \n",
       "3289        0  [-0.060306523, 0.06860288, 0.31759515, 0.22624...   \n",
       "3290        0  [0.009814276, 0.013060865, 0.09264118, 0.71502...   \n",
       "3291        0  [-0.50167626, 0.11863858, 0.26922804, -0.39989...   \n",
       "\n",
       "                                      description_token  \\\n",
       "0     [name, english, wild, flower, meadow, amli, de...   \n",
       "1     [vice, reversa, aim, focu, target, deliveri, s...   \n",
       "2     [jean, arcel, renovar, collect, said, featur, ...   \n",
       "3     [vice, reversa, aim, focu, target, deliveri, s...   \n",
       "4     [oil, product, skin, hair, creat, exclus, ecol...   \n",
       "...                                                 ...   \n",
       "3287  [edeka, elko, bodi, bodi, lotion, relaunch, ne...   \n",
       "3288  [seara, argila, natur, especi, em, pó, special...   \n",
       "3289  [amalfi, skin, care, leit, corpor, coenzima, c...   \n",
       "3290  [amalfi, skin, care, rosa, mosqueta, lech, cor...   \n",
       "3291  [sandi, gül, suyu, rose, water, use, tonic, sk...   \n",
       "\n",
       "                                         description_wv  \n",
       "0     [[-0.051982604, -0.08290577, 0.3159616, 0.0085...  \n",
       "1     [[0.3694772, -0.57862556, -2.0269456, -0.64512...  \n",
       "2     [[0.8480468, -0.055171926, -0.54320484, 0.1345...  \n",
       "3     [[0.3694772, -0.57862556, -2.0269456, -0.64512...  \n",
       "4     [[0.9449418, -0.19407645, 0.33627698, -1.56030...  \n",
       "...                                                 ...  \n",
       "3287  [[0.16146915, 0.81466347, 0.26688766, 1.377136...  \n",
       "3288  [[0.24507636, 0.19971885, 0.1813712, 0.0929433...  \n",
       "3289  [[-1.9248099, 0.80903107, 0.48631546, 0.015191...  \n",
       "3290  [[-1.9248099, 0.80903107, 0.48631546, 0.015191...  \n",
       "3291  [[-0.1723986, 0.033396434, 0.74005365, -0.0430...  \n",
       "\n",
       "[3292 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at cosine similarity Premium vs Premium, Premium vs Non-Premium, Non-Premium vs Non-Premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18499793"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highest Premium Brand vs Lowest Non-Premium Brand in term of unit_price\n",
    "a = df['doc_vec'].iloc[0]\n",
    "b = df['doc_vec'].iloc[3291]\n",
    "np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20241445"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 1 Premium Brand vs Top 2 Premium Brand in term of unit_price\n",
    "a = df['doc_vec'].iloc[0]\n",
    "b = df['doc_vec'].iloc[1]\n",
    "np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17533217"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowest Premium Brand vs Second lowest Premium Brand in term of unit_price\n",
    "a = df['doc_vec'].iloc[3290]\n",
    "b = df['doc_vec'].iloc[3291]\n",
    "np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between above cosine similarity are minimal. It means that the document vectors above are similar. However, the document vectors are just the input of the prediction model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Premium_NonPremium = df['premium']\n",
    "Doc_Vec = df['doc_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Vec_array = np.array(Doc_Vec.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Doc_Vec_array, Premium_NonPremium, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33352196e+00,  2.40045171e-02, -1.66448310e-01, ...,\n",
       "        -7.33567059e-01,  3.27428937e-01, -3.71708602e-01],\n",
       "       [-4.39466625e-01,  1.82640761e-01,  1.06200898e+00, ...,\n",
       "         2.69202024e-01, -1.33528829e+00, -7.87972569e-01],\n",
       "       [ 8.49326432e-01,  9.40741003e-01, -1.84730247e-01, ...,\n",
       "        -2.49549642e-01, -2.94388771e+00, -1.54712903e+00],\n",
       "       ...,\n",
       "       [-7.08502352e-01,  4.72000152e-01,  7.35374331e-01, ...,\n",
       "         5.68219900e-01,  4.98221248e-01,  4.93306145e-02],\n",
       "       [-1.30063444e-01, -3.63060951e-01, -6.21677160e-01, ...,\n",
       "        -1.21646285e-01,  2.06929818e-01,  1.68720179e-03],\n",
       "       [ 6.70225024e-01,  4.34521288e-01,  5.12650251e-01, ...,\n",
       "         4.96449679e-01,  5.39018869e-01,  4.81423527e-01]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2633, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2633"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2633"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores [0.95833333 0.95454545 0.95454545 0.95454545 0.95437262 0.95437262\n",
      " 0.9581749  0.96197719 0.9391635  0.97328244]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\we704\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(logreg, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9391634980988594, 0.956331297621969, 0.9732824427480916)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.Series(scores)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9423368740515933\n",
      "Testing F1 score: 0.9423419238275568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred_log))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred_log, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = ['Budget', 'Premium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(21.453125, 0.5, 'predicted label')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred_log)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels = axis, yticklabels = axis)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[301,  18],\n",
       "       [ 20, 320]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_array = np.array(description.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(description_array, Premium_NonPremium, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "NB_model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model.fit(X_train, y_train)\n",
    "y_pred_nb = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores [0.98863636 0.98106061 0.98863636 0.98106061 0.96577947 0.99239544\n",
      " 0.98859316 0.98859316 0.97718631 0.99618321]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(NB_model, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9657794676806084, 0.9848124674017921, 0.9961832061068703)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.Series(scores)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9711684370257967\n",
      "Testing F1 score: 0.9711390146862073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred_nb))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred_nb, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(21.453125, 0.5, 'predicted label')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred_nb)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels = axis, yticklabels = axis)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[303,  16],\n",
       "       [  3, 337]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Doc_Vec_array, Premium_NonPremium, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47188884\n",
      "Iteration 2, loss = 0.19780477\n",
      "Iteration 3, loss = 0.12677432\n",
      "Iteration 4, loss = 0.09565694\n",
      "Iteration 5, loss = 0.07745176\n",
      "Iteration 6, loss = 0.06283955\n",
      "Iteration 7, loss = 0.05325674\n",
      "Iteration 8, loss = 0.04567673\n",
      "Iteration 9, loss = 0.03993266\n",
      "Iteration 10, loss = 0.03519552\n",
      "Iteration 11, loss = 0.03097189\n",
      "Iteration 12, loss = 0.02781145\n",
      "Iteration 13, loss = 0.02519471\n",
      "Iteration 14, loss = 0.02308248\n",
      "Iteration 15, loss = 0.02067237\n",
      "Iteration 16, loss = 0.01903714\n",
      "Iteration 17, loss = 0.01820017\n",
      "Iteration 18, loss = 0.01615006\n",
      "Iteration 19, loss = 0.01475710\n",
      "Iteration 20, loss = 0.01362111\n",
      "Iteration 21, loss = 0.01258300\n",
      "Iteration 22, loss = 0.01147995\n",
      "Iteration 23, loss = 0.01068131\n",
      "Iteration 24, loss = 0.00989340\n",
      "Iteration 25, loss = 0.00921479\n",
      "Iteration 26, loss = 0.00854938\n",
      "Iteration 27, loss = 0.00797392\n",
      "Iteration 28, loss = 0.00749300\n",
      "Iteration 29, loss = 0.00693899\n",
      "Iteration 30, loss = 0.00647885\n",
      "Iteration 31, loss = 0.00610266\n",
      "Iteration 32, loss = 0.00598451\n",
      "Iteration 33, loss = 0.00549583\n",
      "Iteration 34, loss = 0.00515561\n",
      "Iteration 35, loss = 0.00478193\n",
      "Iteration 36, loss = 0.00437328\n",
      "Iteration 37, loss = 0.00405940\n",
      "Iteration 38, loss = 0.00382014\n",
      "Iteration 39, loss = 0.00360524\n",
      "Iteration 40, loss = 0.00340319\n",
      "Iteration 41, loss = 0.00319824\n",
      "Iteration 42, loss = 0.00303168\n",
      "Iteration 43, loss = 0.00290474\n",
      "Iteration 44, loss = 0.00276842\n",
      "Iteration 45, loss = 0.00262287\n",
      "Iteration 46, loss = 0.00250875\n",
      "Iteration 47, loss = 0.00240496\n",
      "Iteration 48, loss = 0.00230592\n",
      "Iteration 49, loss = 0.00219055\n",
      "Iteration 50, loss = 0.00210728\n",
      "Iteration 51, loss = 0.00201606\n",
      "Iteration 52, loss = 0.00193461\n",
      "Iteration 53, loss = 0.00186008\n",
      "Iteration 54, loss = 0.00178757\n",
      "Iteration 55, loss = 0.00172960\n",
      "Iteration 56, loss = 0.00165702\n",
      "Iteration 57, loss = 0.00159973\n",
      "Iteration 58, loss = 0.00153987\n",
      "Iteration 59, loss = 0.00149388\n",
      "Iteration 60, loss = 0.00143605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf_V1 = MLPClassifier(hidden_layer_sizes = (100), verbose = True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58766394\n",
      "Iteration 2, loss = 0.28302798\n",
      "Iteration 3, loss = 0.18000706\n",
      "Iteration 4, loss = 0.13110274\n",
      "Iteration 5, loss = 0.10324668\n",
      "Iteration 6, loss = 0.08467110\n",
      "Iteration 7, loss = 0.07126627\n",
      "Iteration 8, loss = 0.06071655\n",
      "Iteration 9, loss = 0.05255930\n",
      "Iteration 10, loss = 0.04647973\n",
      "Iteration 11, loss = 0.04103166\n",
      "Iteration 12, loss = 0.03682285\n",
      "Iteration 13, loss = 0.03292072\n",
      "Iteration 14, loss = 0.02954141\n",
      "Iteration 15, loss = 0.02683738\n",
      "Iteration 16, loss = 0.02428461\n",
      "Iteration 17, loss = 0.02238692\n",
      "Iteration 18, loss = 0.02037415\n",
      "Iteration 19, loss = 0.01879224\n",
      "Iteration 20, loss = 0.01740434\n",
      "Iteration 21, loss = 0.01600059\n",
      "Iteration 22, loss = 0.01493603\n",
      "Iteration 23, loss = 0.01395300\n",
      "Iteration 24, loss = 0.01283998\n",
      "Iteration 25, loss = 0.01201155\n",
      "Iteration 26, loss = 0.01138279\n",
      "Iteration 27, loss = 0.01050509\n",
      "Iteration 28, loss = 0.00982439\n",
      "Iteration 29, loss = 0.00922596\n",
      "Iteration 30, loss = 0.00862713\n",
      "Iteration 31, loss = 0.00814283\n",
      "Iteration 32, loss = 0.00760142\n",
      "Iteration 33, loss = 0.00718962\n",
      "Iteration 34, loss = 0.00678209\n",
      "Iteration 35, loss = 0.00634877\n",
      "Iteration 36, loss = 0.00598394\n",
      "Iteration 37, loss = 0.00565311\n",
      "Iteration 38, loss = 0.00534624\n",
      "Iteration 39, loss = 0.00507465\n",
      "Iteration 40, loss = 0.00478341\n",
      "Iteration 41, loss = 0.00455445\n",
      "Iteration 42, loss = 0.00428922\n",
      "Iteration 43, loss = 0.00409998\n",
      "Iteration 44, loss = 0.00392112\n",
      "Iteration 45, loss = 0.00371681\n",
      "Iteration 46, loss = 0.00352943\n",
      "Iteration 47, loss = 0.00337636\n",
      "Iteration 48, loss = 0.00321825\n",
      "Iteration 49, loss = 0.00307632\n",
      "Iteration 50, loss = 0.00295753\n",
      "Iteration 51, loss = 0.00283558\n",
      "Iteration 52, loss = 0.00275076\n",
      "Iteration 53, loss = 0.00261887\n",
      "Iteration 54, loss = 0.00250128\n",
      "Iteration 55, loss = 0.00238954\n",
      "Iteration 56, loss = 0.00229335\n",
      "Iteration 57, loss = 0.00220627\n",
      "Iteration 58, loss = 0.00212521\n",
      "Iteration 59, loss = 0.00204866\n",
      "Iteration 60, loss = 0.00198330\n",
      "Iteration 61, loss = 0.00190561\n",
      "Iteration 62, loss = 0.00183948\n",
      "Iteration 63, loss = 0.00177171\n",
      "Iteration 64, loss = 0.00171203\n",
      "Iteration 65, loss = 0.00166102\n",
      "Iteration 66, loss = 0.00160581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47718656\n",
      "Iteration 2, loss = 0.21583889\n",
      "Iteration 3, loss = 0.14000857\n",
      "Iteration 4, loss = 0.10381513\n",
      "Iteration 5, loss = 0.08275622\n",
      "Iteration 6, loss = 0.06878752\n",
      "Iteration 7, loss = 0.05826911\n",
      "Iteration 8, loss = 0.05036664\n",
      "Iteration 9, loss = 0.04355057\n",
      "Iteration 10, loss = 0.03856181\n",
      "Iteration 11, loss = 0.03439452\n",
      "Iteration 12, loss = 0.03101494\n",
      "Iteration 13, loss = 0.02778026\n",
      "Iteration 14, loss = 0.02537727\n",
      "Iteration 15, loss = 0.02304138\n",
      "Iteration 16, loss = 0.02122442\n",
      "Iteration 17, loss = 0.01955625\n",
      "Iteration 18, loss = 0.01803036\n",
      "Iteration 19, loss = 0.01670603\n",
      "Iteration 20, loss = 0.01548831\n",
      "Iteration 21, loss = 0.01461175\n",
      "Iteration 22, loss = 0.01359897\n",
      "Iteration 23, loss = 0.01246671\n",
      "Iteration 24, loss = 0.01165230\n",
      "Iteration 25, loss = 0.01085471\n",
      "Iteration 26, loss = 0.01015229\n",
      "Iteration 27, loss = 0.00945955\n",
      "Iteration 28, loss = 0.00889304\n",
      "Iteration 29, loss = 0.00837613\n",
      "Iteration 30, loss = 0.00779720\n",
      "Iteration 31, loss = 0.00730037\n",
      "Iteration 32, loss = 0.00687863\n",
      "Iteration 33, loss = 0.00647592\n",
      "Iteration 34, loss = 0.00610002\n",
      "Iteration 35, loss = 0.00573970\n",
      "Iteration 36, loss = 0.00542501\n",
      "Iteration 37, loss = 0.00506104\n",
      "Iteration 38, loss = 0.00482582\n",
      "Iteration 39, loss = 0.00457037\n",
      "Iteration 40, loss = 0.00430971\n",
      "Iteration 41, loss = 0.00408193\n",
      "Iteration 42, loss = 0.00390294\n",
      "Iteration 43, loss = 0.00369354\n",
      "Iteration 44, loss = 0.00352038\n",
      "Iteration 45, loss = 0.00335105\n",
      "Iteration 46, loss = 0.00318783\n",
      "Iteration 47, loss = 0.00305925\n",
      "Iteration 48, loss = 0.00292297\n",
      "Iteration 49, loss = 0.00279088\n",
      "Iteration 50, loss = 0.00267845\n",
      "Iteration 51, loss = 0.00256346\n",
      "Iteration 52, loss = 0.00247158\n",
      "Iteration 53, loss = 0.00235689\n",
      "Iteration 54, loss = 0.00226939\n",
      "Iteration 55, loss = 0.00216940\n",
      "Iteration 56, loss = 0.00208620\n",
      "Iteration 57, loss = 0.00200629\n",
      "Iteration 58, loss = 0.00193036\n",
      "Iteration 59, loss = 0.00186541\n",
      "Iteration 60, loss = 0.00179654\n",
      "Iteration 61, loss = 0.00173471\n",
      "Iteration 62, loss = 0.00167818\n",
      "Iteration 63, loss = 0.00161984\n",
      "Iteration 64, loss = 0.00156344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52631556\n",
      "Iteration 2, loss = 0.23326280\n",
      "Iteration 3, loss = 0.14410381\n",
      "Iteration 4, loss = 0.10495273\n",
      "Iteration 5, loss = 0.08327167\n",
      "Iteration 6, loss = 0.06838788\n",
      "Iteration 7, loss = 0.05797567\n",
      "Iteration 8, loss = 0.05026651\n",
      "Iteration 9, loss = 0.04326328\n",
      "Iteration 10, loss = 0.03829796\n",
      "Iteration 11, loss = 0.03450710\n",
      "Iteration 12, loss = 0.03074026\n",
      "Iteration 13, loss = 0.02785403\n",
      "Iteration 14, loss = 0.02508491\n",
      "Iteration 15, loss = 0.02302795\n",
      "Iteration 16, loss = 0.02116464\n",
      "Iteration 17, loss = 0.01940529\n",
      "Iteration 18, loss = 0.01823682\n",
      "Iteration 19, loss = 0.01655152\n",
      "Iteration 20, loss = 0.01538834\n",
      "Iteration 21, loss = 0.01437581\n",
      "Iteration 22, loss = 0.01341766\n",
      "Iteration 23, loss = 0.01255473\n",
      "Iteration 24, loss = 0.01158607\n",
      "Iteration 25, loss = 0.01084148\n",
      "Iteration 26, loss = 0.01013666\n",
      "Iteration 27, loss = 0.00948195\n",
      "Iteration 28, loss = 0.00881996\n",
      "Iteration 29, loss = 0.00828065\n",
      "Iteration 30, loss = 0.00775384\n",
      "Iteration 31, loss = 0.00745722\n",
      "Iteration 32, loss = 0.00686306\n",
      "Iteration 33, loss = 0.00637071\n",
      "Iteration 34, loss = 0.00605986\n",
      "Iteration 35, loss = 0.00566132\n",
      "Iteration 36, loss = 0.00544595\n",
      "Iteration 37, loss = 0.00505026\n",
      "Iteration 38, loss = 0.00475105\n",
      "Iteration 39, loss = 0.00450467\n",
      "Iteration 40, loss = 0.00429094\n",
      "Iteration 41, loss = 0.00405866\n",
      "Iteration 42, loss = 0.00385070\n",
      "Iteration 43, loss = 0.00367783\n",
      "Iteration 44, loss = 0.00348872\n",
      "Iteration 45, loss = 0.00333294\n",
      "Iteration 46, loss = 0.00315759\n",
      "Iteration 47, loss = 0.00302733\n",
      "Iteration 48, loss = 0.00289262\n",
      "Iteration 49, loss = 0.00275864\n",
      "Iteration 50, loss = 0.00265938\n",
      "Iteration 51, loss = 0.00253706\n",
      "Iteration 52, loss = 0.00243171\n",
      "Iteration 53, loss = 0.00233634\n",
      "Iteration 54, loss = 0.00224768\n",
      "Iteration 55, loss = 0.00215781\n",
      "Iteration 56, loss = 0.00207739\n",
      "Iteration 57, loss = 0.00199711\n",
      "Iteration 58, loss = 0.00193550\n",
      "Iteration 59, loss = 0.00185671\n",
      "Iteration 60, loss = 0.00179810\n",
      "Iteration 61, loss = 0.00173803\n",
      "Iteration 62, loss = 0.00167512\n",
      "Iteration 63, loss = 0.00161276\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53900470\n",
      "Iteration 2, loss = 0.23088168\n",
      "Iteration 3, loss = 0.14517501\n",
      "Iteration 4, loss = 0.10819778\n",
      "Iteration 5, loss = 0.08656283\n",
      "Iteration 6, loss = 0.07146722\n",
      "Iteration 7, loss = 0.06068068\n",
      "Iteration 8, loss = 0.05188935\n",
      "Iteration 9, loss = 0.04520846\n",
      "Iteration 10, loss = 0.04000588\n",
      "Iteration 11, loss = 0.03544595\n",
      "Iteration 12, loss = 0.03155663\n",
      "Iteration 13, loss = 0.02849637\n",
      "Iteration 14, loss = 0.02589284\n",
      "Iteration 15, loss = 0.02337153\n",
      "Iteration 16, loss = 0.02135263\n",
      "Iteration 17, loss = 0.01969735\n",
      "Iteration 18, loss = 0.01794192\n",
      "Iteration 19, loss = 0.01662513\n",
      "Iteration 20, loss = 0.01535825\n",
      "Iteration 21, loss = 0.01412770\n",
      "Iteration 22, loss = 0.01332286\n",
      "Iteration 23, loss = 0.01223094\n",
      "Iteration 24, loss = 0.01143871\n",
      "Iteration 25, loss = 0.01061556\n",
      "Iteration 26, loss = 0.00989018\n",
      "Iteration 27, loss = 0.00920771\n",
      "Iteration 28, loss = 0.00859424\n",
      "Iteration 29, loss = 0.00806431\n",
      "Iteration 30, loss = 0.00758029\n",
      "Iteration 31, loss = 0.00713767\n",
      "Iteration 32, loss = 0.00664123\n",
      "Iteration 33, loss = 0.00628668\n",
      "Iteration 34, loss = 0.00583347\n",
      "Iteration 35, loss = 0.00551978\n",
      "Iteration 36, loss = 0.00519521\n",
      "Iteration 37, loss = 0.00490365\n",
      "Iteration 38, loss = 0.00464531\n",
      "Iteration 39, loss = 0.00441309\n",
      "Iteration 40, loss = 0.00418376\n",
      "Iteration 41, loss = 0.00392994\n",
      "Iteration 42, loss = 0.00375447\n",
      "Iteration 43, loss = 0.00356320\n",
      "Iteration 44, loss = 0.00337549\n",
      "Iteration 45, loss = 0.00323735\n",
      "Iteration 46, loss = 0.00308568\n",
      "Iteration 47, loss = 0.00295494\n",
      "Iteration 48, loss = 0.00280302\n",
      "Iteration 49, loss = 0.00270027\n",
      "Iteration 50, loss = 0.00258529\n",
      "Iteration 51, loss = 0.00246230\n",
      "Iteration 52, loss = 0.00238174\n",
      "Iteration 53, loss = 0.00227817\n",
      "Iteration 54, loss = 0.00219559\n",
      "Iteration 55, loss = 0.00210989\n",
      "Iteration 56, loss = 0.00202568\n",
      "Iteration 57, loss = 0.00194486\n",
      "Iteration 58, loss = 0.00187309\n",
      "Iteration 59, loss = 0.00181004\n",
      "Iteration 60, loss = 0.00174931\n",
      "Iteration 61, loss = 0.00168852\n",
      "Iteration 62, loss = 0.00162524\n",
      "Iteration 63, loss = 0.00158440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 0.00152878\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52050997\n",
      "Iteration 2, loss = 0.24902882\n",
      "Iteration 3, loss = 0.15358025\n",
      "Iteration 4, loss = 0.11157420\n",
      "Iteration 5, loss = 0.08759655\n",
      "Iteration 6, loss = 0.07114667\n",
      "Iteration 7, loss = 0.05928777\n",
      "Iteration 8, loss = 0.05061946\n",
      "Iteration 9, loss = 0.04380480\n",
      "Iteration 10, loss = 0.03818555\n",
      "Iteration 11, loss = 0.03433246\n",
      "Iteration 12, loss = 0.03033991\n",
      "Iteration 13, loss = 0.02720705\n",
      "Iteration 14, loss = 0.02461155\n",
      "Iteration 15, loss = 0.02245003\n",
      "Iteration 16, loss = 0.02063000\n",
      "Iteration 17, loss = 0.01879568\n",
      "Iteration 18, loss = 0.01726642\n",
      "Iteration 19, loss = 0.01610551\n",
      "Iteration 20, loss = 0.01489505\n",
      "Iteration 21, loss = 0.01380661\n",
      "Iteration 22, loss = 0.01279925\n",
      "Iteration 23, loss = 0.01191146\n",
      "Iteration 24, loss = 0.01109527\n",
      "Iteration 25, loss = 0.01035840\n",
      "Iteration 26, loss = 0.00969386\n",
      "Iteration 27, loss = 0.00899348\n",
      "Iteration 28, loss = 0.00834493\n",
      "Iteration 29, loss = 0.00782077\n",
      "Iteration 30, loss = 0.00730444\n",
      "Iteration 31, loss = 0.00687953\n",
      "Iteration 32, loss = 0.00640910\n",
      "Iteration 33, loss = 0.00604051\n",
      "Iteration 34, loss = 0.00568041\n",
      "Iteration 35, loss = 0.00536761\n",
      "Iteration 36, loss = 0.00502342\n",
      "Iteration 37, loss = 0.00472979\n",
      "Iteration 38, loss = 0.00444807\n",
      "Iteration 39, loss = 0.00421696\n",
      "Iteration 40, loss = 0.00397404\n",
      "Iteration 41, loss = 0.00377342\n",
      "Iteration 42, loss = 0.00358516\n",
      "Iteration 43, loss = 0.00340367\n",
      "Iteration 44, loss = 0.00326085\n",
      "Iteration 45, loss = 0.00309207\n",
      "Iteration 46, loss = 0.00294840\n",
      "Iteration 47, loss = 0.00284307\n",
      "Iteration 48, loss = 0.00268008\n",
      "Iteration 49, loss = 0.00257517\n",
      "Iteration 50, loss = 0.00246590\n",
      "Iteration 51, loss = 0.00238520\n",
      "Iteration 52, loss = 0.00227429\n",
      "Iteration 53, loss = 0.00217587\n",
      "Iteration 54, loss = 0.00208898\n",
      "Iteration 55, loss = 0.00200363\n",
      "Iteration 56, loss = 0.00193086\n",
      "Iteration 57, loss = 0.00185637\n",
      "Iteration 58, loss = 0.00179741\n",
      "Iteration 59, loss = 0.00172693\n",
      "Iteration 60, loss = 0.00166739\n",
      "Iteration 61, loss = 0.00160887\n",
      "Iteration 62, loss = 0.00155712\n",
      "Iteration 63, loss = 0.00150593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50241958\n",
      "Iteration 2, loss = 0.21407498\n",
      "Iteration 3, loss = 0.13544768\n",
      "Iteration 4, loss = 0.10041081\n",
      "Iteration 5, loss = 0.07894380\n",
      "Iteration 6, loss = 0.06523414\n",
      "Iteration 7, loss = 0.05439828\n",
      "Iteration 8, loss = 0.04635987\n",
      "Iteration 9, loss = 0.04013788\n",
      "Iteration 10, loss = 0.03511614\n",
      "Iteration 11, loss = 0.03076123\n",
      "Iteration 12, loss = 0.02737240\n",
      "Iteration 13, loss = 0.02449653\n",
      "Iteration 14, loss = 0.02177709\n",
      "Iteration 15, loss = 0.01972711\n",
      "Iteration 16, loss = 0.01786939\n",
      "Iteration 17, loss = 0.01634054\n",
      "Iteration 18, loss = 0.01486120\n",
      "Iteration 19, loss = 0.01358796\n",
      "Iteration 20, loss = 0.01239142\n",
      "Iteration 21, loss = 0.01139001\n",
      "Iteration 22, loss = 0.01059560\n",
      "Iteration 23, loss = 0.00967337\n",
      "Iteration 24, loss = 0.00898821\n",
      "Iteration 25, loss = 0.00837609\n",
      "Iteration 26, loss = 0.00771461\n",
      "Iteration 27, loss = 0.00716023\n",
      "Iteration 28, loss = 0.00671715\n",
      "Iteration 29, loss = 0.00620411\n",
      "Iteration 30, loss = 0.00582696\n",
      "Iteration 31, loss = 0.00542092\n",
      "Iteration 32, loss = 0.00508492\n",
      "Iteration 33, loss = 0.00476177\n",
      "Iteration 34, loss = 0.00448198\n",
      "Iteration 35, loss = 0.00421372\n",
      "Iteration 36, loss = 0.00398517\n",
      "Iteration 37, loss = 0.00374098\n",
      "Iteration 38, loss = 0.00354120\n",
      "Iteration 39, loss = 0.00335658\n",
      "Iteration 40, loss = 0.00316531\n",
      "Iteration 41, loss = 0.00301188\n",
      "Iteration 42, loss = 0.00285935\n",
      "Iteration 43, loss = 0.00271111\n",
      "Iteration 44, loss = 0.00258667\n",
      "Iteration 45, loss = 0.00246601\n",
      "Iteration 46, loss = 0.00235450\n",
      "Iteration 47, loss = 0.00225009\n",
      "Iteration 48, loss = 0.00215443\n",
      "Iteration 49, loss = 0.00205634\n",
      "Iteration 50, loss = 0.00197759\n",
      "Iteration 51, loss = 0.00189696\n",
      "Iteration 52, loss = 0.00182410\n",
      "Iteration 53, loss = 0.00174974\n",
      "Iteration 54, loss = 0.00168222\n",
      "Iteration 55, loss = 0.00162468\n",
      "Iteration 56, loss = 0.00157182\n",
      "Iteration 57, loss = 0.00150512\n",
      "Iteration 58, loss = 0.00145072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53218501\n",
      "Iteration 2, loss = 0.24240129\n",
      "Iteration 3, loss = 0.15105225\n",
      "Iteration 4, loss = 0.11062756\n",
      "Iteration 5, loss = 0.08842474\n",
      "Iteration 6, loss = 0.07297140\n",
      "Iteration 7, loss = 0.06172060\n",
      "Iteration 8, loss = 0.05252223\n",
      "Iteration 9, loss = 0.04586930\n",
      "Iteration 10, loss = 0.04026162\n",
      "Iteration 11, loss = 0.03560914\n",
      "Iteration 12, loss = 0.03174948\n",
      "Iteration 13, loss = 0.02872431\n",
      "Iteration 14, loss = 0.02573419\n",
      "Iteration 15, loss = 0.02349090\n",
      "Iteration 16, loss = 0.02127532\n",
      "Iteration 17, loss = 0.01948967\n",
      "Iteration 18, loss = 0.01797058\n",
      "Iteration 19, loss = 0.01652047\n",
      "Iteration 20, loss = 0.01523918\n",
      "Iteration 21, loss = 0.01422771\n",
      "Iteration 22, loss = 0.01302733\n",
      "Iteration 23, loss = 0.01206428\n",
      "Iteration 24, loss = 0.01118068\n",
      "Iteration 25, loss = 0.01043444\n",
      "Iteration 26, loss = 0.00975218\n",
      "Iteration 27, loss = 0.00916816\n",
      "Iteration 28, loss = 0.00849554\n",
      "Iteration 29, loss = 0.00795731\n",
      "Iteration 30, loss = 0.00747826\n",
      "Iteration 31, loss = 0.00703783\n",
      "Iteration 32, loss = 0.00660844\n",
      "Iteration 33, loss = 0.00620893\n",
      "Iteration 34, loss = 0.00584425\n",
      "Iteration 35, loss = 0.00552137\n",
      "Iteration 36, loss = 0.00522040\n",
      "Iteration 37, loss = 0.00494063\n",
      "Iteration 38, loss = 0.00465759\n",
      "Iteration 39, loss = 0.00444790\n",
      "Iteration 40, loss = 0.00422097\n",
      "Iteration 41, loss = 0.00399016\n",
      "Iteration 42, loss = 0.00381250\n",
      "Iteration 43, loss = 0.00363985\n",
      "Iteration 44, loss = 0.00345984\n",
      "Iteration 45, loss = 0.00329651\n",
      "Iteration 46, loss = 0.00314357\n",
      "Iteration 47, loss = 0.00302716\n",
      "Iteration 48, loss = 0.00287753\n",
      "Iteration 49, loss = 0.00278117\n",
      "Iteration 50, loss = 0.00264143\n",
      "Iteration 51, loss = 0.00253274\n",
      "Iteration 52, loss = 0.00243452\n",
      "Iteration 53, loss = 0.00233973\n",
      "Iteration 54, loss = 0.00224974\n",
      "Iteration 55, loss = 0.00216871\n",
      "Iteration 56, loss = 0.00207981\n",
      "Iteration 57, loss = 0.00200901\n",
      "Iteration 58, loss = 0.00193519\n",
      "Iteration 59, loss = 0.00186985\n",
      "Iteration 60, loss = 0.00180664\n",
      "Iteration 61, loss = 0.00173658\n",
      "Iteration 62, loss = 0.00168895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48444696\n",
      "Iteration 2, loss = 0.20460689\n",
      "Iteration 3, loss = 0.12847194\n",
      "Iteration 4, loss = 0.09641740\n",
      "Iteration 5, loss = 0.07710473\n",
      "Iteration 6, loss = 0.06407006\n",
      "Iteration 7, loss = 0.05438546\n",
      "Iteration 8, loss = 0.04685104\n",
      "Iteration 9, loss = 0.04090788\n",
      "Iteration 10, loss = 0.03621257\n",
      "Iteration 11, loss = 0.03231731\n",
      "Iteration 12, loss = 0.02908453\n",
      "Iteration 13, loss = 0.02594530\n",
      "Iteration 14, loss = 0.02374577\n",
      "Iteration 15, loss = 0.02174599\n",
      "Iteration 16, loss = 0.01965131\n",
      "Iteration 17, loss = 0.01856259\n",
      "Iteration 18, loss = 0.01682094\n",
      "Iteration 19, loss = 0.01533572\n",
      "Iteration 20, loss = 0.01437817\n",
      "Iteration 21, loss = 0.01323402\n",
      "Iteration 22, loss = 0.01228082\n",
      "Iteration 23, loss = 0.01135586\n",
      "Iteration 24, loss = 0.01054983\n",
      "Iteration 25, loss = 0.00989242\n",
      "Iteration 26, loss = 0.00923683\n",
      "Iteration 27, loss = 0.00854890\n",
      "Iteration 28, loss = 0.00797978\n",
      "Iteration 29, loss = 0.00746229\n",
      "Iteration 30, loss = 0.00694175\n",
      "Iteration 31, loss = 0.00651340\n",
      "Iteration 32, loss = 0.00608950\n",
      "Iteration 33, loss = 0.00574530\n",
      "Iteration 34, loss = 0.00536220\n",
      "Iteration 35, loss = 0.00504747\n",
      "Iteration 36, loss = 0.00480616\n",
      "Iteration 37, loss = 0.00451756\n",
      "Iteration 38, loss = 0.00425160\n",
      "Iteration 39, loss = 0.00402555\n",
      "Iteration 40, loss = 0.00379438\n",
      "Iteration 41, loss = 0.00363317\n",
      "Iteration 42, loss = 0.00343456\n",
      "Iteration 43, loss = 0.00326981\n",
      "Iteration 44, loss = 0.00310871\n",
      "Iteration 45, loss = 0.00296968\n",
      "Iteration 46, loss = 0.00283161\n",
      "Iteration 47, loss = 0.00273402\n",
      "Iteration 48, loss = 0.00259411\n",
      "Iteration 49, loss = 0.00249474\n",
      "Iteration 50, loss = 0.00236813\n",
      "Iteration 51, loss = 0.00225688\n",
      "Iteration 52, loss = 0.00216018\n",
      "Iteration 53, loss = 0.00207056\n",
      "Iteration 54, loss = 0.00198981\n",
      "Iteration 55, loss = 0.00191134\n",
      "Iteration 56, loss = 0.00183873\n",
      "Iteration 57, loss = 0.00177116\n",
      "Iteration 58, loss = 0.00170560\n",
      "Iteration 59, loss = 0.00164610\n",
      "Iteration 60, loss = 0.00158755\n",
      "Iteration 61, loss = 0.00153359\n",
      "Iteration 62, loss = 0.00148675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57549744\n",
      "Iteration 2, loss = 0.25751823\n",
      "Iteration 3, loss = 0.16139897\n",
      "Iteration 4, loss = 0.12025618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.09571892\n",
      "Iteration 6, loss = 0.07918392\n",
      "Iteration 7, loss = 0.06705744\n",
      "Iteration 8, loss = 0.05704141\n",
      "Iteration 9, loss = 0.04985246\n",
      "Iteration 10, loss = 0.04360702\n",
      "Iteration 11, loss = 0.03871144\n",
      "Iteration 12, loss = 0.03464963\n",
      "Iteration 13, loss = 0.03116846\n",
      "Iteration 14, loss = 0.02823784\n",
      "Iteration 15, loss = 0.02570420\n",
      "Iteration 16, loss = 0.02347640\n",
      "Iteration 17, loss = 0.02150608\n",
      "Iteration 18, loss = 0.01973674\n",
      "Iteration 19, loss = 0.01821628\n",
      "Iteration 20, loss = 0.01686867\n",
      "Iteration 21, loss = 0.01563042\n",
      "Iteration 22, loss = 0.01451030\n",
      "Iteration 23, loss = 0.01346583\n",
      "Iteration 24, loss = 0.01255723\n",
      "Iteration 25, loss = 0.01168979\n",
      "Iteration 26, loss = 0.01090333\n",
      "Iteration 27, loss = 0.01024207\n",
      "Iteration 28, loss = 0.00961510\n",
      "Iteration 29, loss = 0.00895814\n",
      "Iteration 30, loss = 0.00838435\n",
      "Iteration 31, loss = 0.00790260\n",
      "Iteration 32, loss = 0.00741282\n",
      "Iteration 33, loss = 0.00691017\n",
      "Iteration 34, loss = 0.00651917\n",
      "Iteration 35, loss = 0.00613241\n",
      "Iteration 36, loss = 0.00576484\n",
      "Iteration 37, loss = 0.00548328\n",
      "Iteration 38, loss = 0.00522483\n",
      "Iteration 39, loss = 0.00488999\n",
      "Iteration 40, loss = 0.00462084\n",
      "Iteration 41, loss = 0.00435594\n",
      "Iteration 42, loss = 0.00411943\n",
      "Iteration 43, loss = 0.00392755\n",
      "Iteration 44, loss = 0.00375040\n",
      "Iteration 45, loss = 0.00355965\n",
      "Iteration 46, loss = 0.00337565\n",
      "Iteration 47, loss = 0.00321075\n",
      "Iteration 48, loss = 0.00306235\n",
      "Iteration 49, loss = 0.00292174\n",
      "Iteration 50, loss = 0.00279360\n",
      "Iteration 51, loss = 0.00267898\n",
      "Iteration 52, loss = 0.00256698\n",
      "Iteration 53, loss = 0.00245468\n",
      "Iteration 54, loss = 0.00236288\n",
      "Iteration 55, loss = 0.00225943\n",
      "Iteration 56, loss = 0.00218926\n",
      "Iteration 57, loss = 0.00208725\n",
      "Iteration 58, loss = 0.00202088\n",
      "Iteration 59, loss = 0.00192903\n",
      "Iteration 60, loss = 0.00186672\n",
      "Iteration 61, loss = 0.00181141\n",
      "Iteration 62, loss = 0.00173556\n",
      "Iteration 63, loss = 0.00168166\n",
      "Iteration 64, loss = 0.00162579\n",
      "Iteration 65, loss = 0.00156385\n",
      "Iteration 66, loss = 0.00151949\n",
      "Iteration 67, loss = 0.00146407\n",
      "Iteration 68, loss = 0.00141798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49005005\n",
      "Iteration 2, loss = 0.22573942\n",
      "Iteration 3, loss = 0.14362146\n",
      "Iteration 4, loss = 0.10647285\n",
      "Iteration 5, loss = 0.08448876\n",
      "Iteration 6, loss = 0.06983456\n",
      "Iteration 7, loss = 0.05883395\n",
      "Iteration 8, loss = 0.05133009\n",
      "Iteration 9, loss = 0.04452868\n",
      "Iteration 10, loss = 0.03941897\n",
      "Iteration 11, loss = 0.03516707\n",
      "Iteration 12, loss = 0.03157714\n",
      "Iteration 13, loss = 0.02830804\n",
      "Iteration 14, loss = 0.02584617\n",
      "Iteration 15, loss = 0.02356027\n",
      "Iteration 16, loss = 0.02149915\n",
      "Iteration 17, loss = 0.01981528\n",
      "Iteration 18, loss = 0.01824987\n",
      "Iteration 19, loss = 0.01701379\n",
      "Iteration 20, loss = 0.01567324\n",
      "Iteration 21, loss = 0.01465497\n",
      "Iteration 22, loss = 0.01353948\n",
      "Iteration 23, loss = 0.01266201\n",
      "Iteration 24, loss = 0.01177551\n",
      "Iteration 25, loss = 0.01087504\n",
      "Iteration 26, loss = 0.01015549\n",
      "Iteration 27, loss = 0.00954652\n",
      "Iteration 28, loss = 0.00880359\n",
      "Iteration 29, loss = 0.00827083\n",
      "Iteration 30, loss = 0.00774256\n",
      "Iteration 31, loss = 0.00722532\n",
      "Iteration 32, loss = 0.00677625\n",
      "Iteration 33, loss = 0.00641939\n",
      "Iteration 34, loss = 0.00592370\n",
      "Iteration 35, loss = 0.00556813\n",
      "Iteration 36, loss = 0.00522563\n",
      "Iteration 37, loss = 0.00495302\n",
      "Iteration 38, loss = 0.00465758\n",
      "Iteration 39, loss = 0.00439499\n",
      "Iteration 40, loss = 0.00415697\n",
      "Iteration 41, loss = 0.00390756\n",
      "Iteration 42, loss = 0.00372134\n",
      "Iteration 43, loss = 0.00354483\n",
      "Iteration 44, loss = 0.00335684\n",
      "Iteration 45, loss = 0.00318646\n",
      "Iteration 46, loss = 0.00303360\n",
      "Iteration 47, loss = 0.00286945\n",
      "Iteration 48, loss = 0.00276011\n",
      "Iteration 49, loss = 0.00262606\n",
      "Iteration 50, loss = 0.00250519\n",
      "Iteration 51, loss = 0.00239659\n",
      "Iteration 52, loss = 0.00229818\n",
      "Iteration 53, loss = 0.00219908\n",
      "Iteration 54, loss = 0.00211255\n",
      "Iteration 55, loss = 0.00201980\n",
      "Iteration 56, loss = 0.00193715\n",
      "Iteration 57, loss = 0.00188794\n",
      "Iteration 58, loss = 0.00179953\n",
      "Iteration 59, loss = 0.00173101\n",
      "Iteration 60, loss = 0.00167578\n",
      "Iteration 61, loss = 0.00160690\n",
      "Iteration 62, loss = 0.00155244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Cross-Validation Accuracy Scores [0.96212121 0.96969697 0.97348485 0.95075758 0.96577947 0.96197719\n",
      " 0.96577947 0.97338403 0.9581749  0.98091603]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf_V1, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9507575757575758, 0.9662071693629178, 0.9809160305343512)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.Series(scores)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clf1 = clf_V1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9514415781487102\n",
      "Testing F1 score: 0.9514491865209518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred_clf1))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred_clf1, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(21.453125, 0.5, 'predicted label')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred_clf1)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels = axis, yticklabels = axis)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[305,  14],\n",
       "       [ 18, 322]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 4 hidden lyaers of 100, 100, 100, 50 units respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.57543050\n",
      "Iteration 2, loss = 0.19110835\n",
      "Iteration 3, loss = 0.09327222\n",
      "Iteration 4, loss = 0.04753377\n",
      "Iteration 5, loss = 0.02956829\n",
      "Iteration 6, loss = 0.01876493\n",
      "Iteration 7, loss = 0.01295301\n",
      "Iteration 8, loss = 0.01153568\n",
      "Iteration 9, loss = 0.00857756\n",
      "Iteration 10, loss = 0.00649849\n",
      "Iteration 11, loss = 0.00422588\n",
      "Iteration 12, loss = 0.00251539\n",
      "Iteration 13, loss = 0.00134016\n",
      "Iteration 14, loss = 0.00092860\n",
      "Iteration 15, loss = 0.00067416\n",
      "Iteration 16, loss = 0.00053355\n",
      "Iteration 17, loss = 0.00043749\n",
      "Iteration 18, loss = 0.00037228\n",
      "Iteration 19, loss = 0.00032920\n",
      "Iteration 20, loss = 0.00029738\n",
      "Iteration 21, loss = 0.00026993\n",
      "Iteration 22, loss = 0.00025011\n",
      "Iteration 23, loss = 0.00023294\n",
      "Iteration 24, loss = 0.00021873\n",
      "Iteration 25, loss = 0.00020739\n",
      "Iteration 26, loss = 0.00019859\n",
      "Iteration 27, loss = 0.00018975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf_V2 = MLPClassifier(hidden_layer_sizes = (100, 100, 100, 50),  verbose = True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53277423\n",
      "Iteration 2, loss = 0.20358576\n",
      "Iteration 3, loss = 0.09412272\n",
      "Iteration 4, loss = 0.05019035\n",
      "Iteration 5, loss = 0.03203578\n",
      "Iteration 6, loss = 0.02012716\n",
      "Iteration 7, loss = 0.01435167\n",
      "Iteration 8, loss = 0.01084833\n",
      "Iteration 9, loss = 0.00832878\n",
      "Iteration 10, loss = 0.00678981\n",
      "Iteration 11, loss = 0.00481199\n",
      "Iteration 12, loss = 0.00327757\n",
      "Iteration 13, loss = 0.00172143\n",
      "Iteration 14, loss = 0.00112603\n",
      "Iteration 15, loss = 0.00082112\n",
      "Iteration 16, loss = 0.00062864\n",
      "Iteration 17, loss = 0.00051674\n",
      "Iteration 18, loss = 0.00044216\n",
      "Iteration 19, loss = 0.00038659\n",
      "Iteration 20, loss = 0.00034608\n",
      "Iteration 21, loss = 0.00031551\n",
      "Iteration 22, loss = 0.00028848\n",
      "Iteration 23, loss = 0.00026896\n",
      "Iteration 24, loss = 0.00025100\n",
      "Iteration 25, loss = 0.00023694\n",
      "Iteration 26, loss = 0.00022379\n",
      "Iteration 27, loss = 0.00021404\n",
      "Iteration 28, loss = 0.00020370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58910240\n",
      "Iteration 2, loss = 0.22735714\n",
      "Iteration 3, loss = 0.09544647\n",
      "Iteration 4, loss = 0.04776544\n",
      "Iteration 5, loss = 0.02704899\n",
      "Iteration 6, loss = 0.01701063\n",
      "Iteration 7, loss = 0.01230047\n",
      "Iteration 8, loss = 0.00848280\n",
      "Iteration 9, loss = 0.00615832\n",
      "Iteration 10, loss = 0.00371247\n",
      "Iteration 11, loss = 0.00205743\n",
      "Iteration 12, loss = 0.00112148\n",
      "Iteration 13, loss = 0.00080578\n",
      "Iteration 14, loss = 0.00059387\n",
      "Iteration 15, loss = 0.00048739\n",
      "Iteration 16, loss = 0.00040015\n",
      "Iteration 17, loss = 0.00034263\n",
      "Iteration 18, loss = 0.00030151\n",
      "Iteration 19, loss = 0.00027623\n",
      "Iteration 20, loss = 0.00025442\n",
      "Iteration 21, loss = 0.00023592\n",
      "Iteration 22, loss = 0.00022154\n",
      "Iteration 23, loss = 0.00020926\n",
      "Iteration 24, loss = 0.00019922\n",
      "Iteration 25, loss = 0.00019031\n",
      "Iteration 26, loss = 0.00018314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55744408\n",
      "Iteration 2, loss = 0.20901617\n",
      "Iteration 3, loss = 0.09141907\n",
      "Iteration 4, loss = 0.05076511\n",
      "Iteration 5, loss = 0.02975132\n",
      "Iteration 6, loss = 0.01955996\n",
      "Iteration 7, loss = 0.01322218\n",
      "Iteration 8, loss = 0.01001899\n",
      "Iteration 9, loss = 0.00691986\n",
      "Iteration 10, loss = 0.00470211\n",
      "Iteration 11, loss = 0.00255642\n",
      "Iteration 12, loss = 0.00147272\n",
      "Iteration 13, loss = 0.00098978\n",
      "Iteration 14, loss = 0.00076432\n",
      "Iteration 15, loss = 0.00060408\n",
      "Iteration 16, loss = 0.00051437\n",
      "Iteration 17, loss = 0.00044191\n",
      "Iteration 18, loss = 0.00039475\n",
      "Iteration 19, loss = 0.00035556\n",
      "Iteration 20, loss = 0.00032277\n",
      "Iteration 21, loss = 0.00029918\n",
      "Iteration 22, loss = 0.00027796\n",
      "Iteration 23, loss = 0.00025942\n",
      "Iteration 24, loss = 0.00024195\n",
      "Iteration 25, loss = 0.00022485\n",
      "Iteration 26, loss = 0.00020763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57748075\n",
      "Iteration 2, loss = 0.22226673\n",
      "Iteration 3, loss = 0.10054064\n",
      "Iteration 4, loss = 0.05189761\n",
      "Iteration 5, loss = 0.02965741\n",
      "Iteration 6, loss = 0.01848685\n",
      "Iteration 7, loss = 0.01274529\n",
      "Iteration 8, loss = 0.00912133\n",
      "Iteration 9, loss = 0.00692410\n",
      "Iteration 10, loss = 0.00468647\n",
      "Iteration 11, loss = 0.00228179\n",
      "Iteration 12, loss = 0.00115708\n",
      "Iteration 13, loss = 0.00079420\n",
      "Iteration 14, loss = 0.00060964\n",
      "Iteration 15, loss = 0.00049173\n",
      "Iteration 16, loss = 0.00041389\n",
      "Iteration 17, loss = 0.00036727\n",
      "Iteration 18, loss = 0.00032795\n",
      "Iteration 19, loss = 0.00029898\n",
      "Iteration 20, loss = 0.00027503\n",
      "Iteration 21, loss = 0.00025637\n",
      "Iteration 22, loss = 0.00024149\n",
      "Iteration 23, loss = 0.00022735\n",
      "Iteration 24, loss = 0.00021672\n",
      "Iteration 25, loss = 0.00020713\n",
      "Iteration 26, loss = 0.00019848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57461035\n",
      "Iteration 2, loss = 0.24001935\n",
      "Iteration 3, loss = 0.10471023\n",
      "Iteration 4, loss = 0.05178992\n",
      "Iteration 5, loss = 0.03201480\n",
      "Iteration 6, loss = 0.02092018\n",
      "Iteration 7, loss = 0.01449956\n",
      "Iteration 8, loss = 0.01093981\n",
      "Iteration 9, loss = 0.00790188\n",
      "Iteration 10, loss = 0.00581702\n",
      "Iteration 11, loss = 0.00367717\n",
      "Iteration 12, loss = 0.00189969\n",
      "Iteration 13, loss = 0.00115424\n",
      "Iteration 14, loss = 0.00081008\n",
      "Iteration 15, loss = 0.00063104\n",
      "Iteration 16, loss = 0.00053507\n",
      "Iteration 17, loss = 0.00042318\n",
      "Iteration 18, loss = 0.00038256\n",
      "Iteration 19, loss = 0.00033375\n",
      "Iteration 20, loss = 0.00030463\n",
      "Iteration 21, loss = 0.00027943\n",
      "Iteration 22, loss = 0.00026034\n",
      "Iteration 23, loss = 0.00024394\n",
      "Iteration 24, loss = 0.00023038\n",
      "Iteration 25, loss = 0.00021782\n",
      "Iteration 26, loss = 0.00020831\n",
      "Iteration 27, loss = 0.00019964\n",
      "Iteration 28, loss = 0.00019202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50775805\n",
      "Iteration 2, loss = 0.17891971\n",
      "Iteration 3, loss = 0.08763999\n",
      "Iteration 4, loss = 0.04382184\n",
      "Iteration 5, loss = 0.02348692\n",
      "Iteration 6, loss = 0.01278306\n",
      "Iteration 7, loss = 0.00805460\n",
      "Iteration 8, loss = 0.00547042\n",
      "Iteration 9, loss = 0.00394619\n",
      "Iteration 10, loss = 0.00246776\n",
      "Iteration 11, loss = 0.00192779\n",
      "Iteration 12, loss = 0.00102209\n",
      "Iteration 13, loss = 0.00067260\n",
      "Iteration 14, loss = 0.00056261\n",
      "Iteration 15, loss = 0.00046541\n",
      "Iteration 16, loss = 0.00040753\n",
      "Iteration 17, loss = 0.00035983\n",
      "Iteration 18, loss = 0.00032410\n",
      "Iteration 19, loss = 0.00029916\n",
      "Iteration 20, loss = 0.00027481\n",
      "Iteration 21, loss = 0.00025716\n",
      "Iteration 22, loss = 0.00024107\n",
      "Iteration 23, loss = 0.00022763\n",
      "Iteration 24, loss = 0.00021701\n",
      "Iteration 25, loss = 0.00020729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59805364\n",
      "Iteration 2, loss = 0.24454110\n",
      "Iteration 3, loss = 0.11169215\n",
      "Iteration 4, loss = 0.05356874\n",
      "Iteration 5, loss = 0.02899030\n",
      "Iteration 6, loss = 0.01715864\n",
      "Iteration 7, loss = 0.01254294\n",
      "Iteration 8, loss = 0.00928855\n",
      "Iteration 9, loss = 0.00743131\n",
      "Iteration 10, loss = 0.00643987\n",
      "Iteration 11, loss = 0.00446191\n",
      "Iteration 12, loss = 0.00283111\n",
      "Iteration 13, loss = 0.00169687\n",
      "Iteration 14, loss = 0.00104752\n",
      "Iteration 15, loss = 0.00075327\n",
      "Iteration 16, loss = 0.00059061\n",
      "Iteration 17, loss = 0.00049259\n",
      "Iteration 18, loss = 0.00041156\n",
      "Iteration 19, loss = 0.00036509\n",
      "Iteration 20, loss = 0.00033087\n",
      "Iteration 21, loss = 0.00030067\n",
      "Iteration 22, loss = 0.00027799\n",
      "Iteration 23, loss = 0.00026061\n",
      "Iteration 24, loss = 0.00024500\n",
      "Iteration 25, loss = 0.00023199\n",
      "Iteration 26, loss = 0.00022113\n",
      "Iteration 27, loss = 0.00021138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59012999\n",
      "Iteration 2, loss = 0.24691853\n",
      "Iteration 3, loss = 0.11078620\n",
      "Iteration 4, loss = 0.06005405\n",
      "Iteration 5, loss = 0.03868877\n",
      "Iteration 6, loss = 0.02660264\n",
      "Iteration 7, loss = 0.01847864\n",
      "Iteration 8, loss = 0.01390905\n",
      "Iteration 9, loss = 0.01034227\n",
      "Iteration 10, loss = 0.00828146\n",
      "Iteration 11, loss = 0.00598619\n",
      "Iteration 12, loss = 0.00404361\n",
      "Iteration 13, loss = 0.00191987\n",
      "Iteration 14, loss = 0.00117800\n",
      "Iteration 15, loss = 0.00083821\n",
      "Iteration 16, loss = 0.00062871\n",
      "Iteration 17, loss = 0.00051420\n",
      "Iteration 18, loss = 0.00043303\n",
      "Iteration 19, loss = 0.00037849\n",
      "Iteration 20, loss = 0.00033959\n",
      "Iteration 21, loss = 0.00031011\n",
      "Iteration 22, loss = 0.00028395\n",
      "Iteration 23, loss = 0.00026172\n",
      "Iteration 24, loss = 0.00024533\n",
      "Iteration 25, loss = 0.00023242\n",
      "Iteration 26, loss = 0.00021951\n",
      "Iteration 27, loss = 0.00021034\n",
      "Iteration 28, loss = 0.00020134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55373819\n",
      "Iteration 2, loss = 0.21060027\n",
      "Iteration 3, loss = 0.10104603\n",
      "Iteration 4, loss = 0.05428086\n",
      "Iteration 5, loss = 0.03296295\n",
      "Iteration 6, loss = 0.02148979\n",
      "Iteration 7, loss = 0.01540291\n",
      "Iteration 8, loss = 0.01153080\n",
      "Iteration 9, loss = 0.00910640\n",
      "Iteration 10, loss = 0.00748653\n",
      "Iteration 11, loss = 0.00480791\n",
      "Iteration 12, loss = 0.00286135\n",
      "Iteration 13, loss = 0.00189186\n",
      "Iteration 14, loss = 0.00103538\n",
      "Iteration 15, loss = 0.00065433\n",
      "Iteration 16, loss = 0.00047261\n",
      "Iteration 17, loss = 0.00040200\n",
      "Iteration 18, loss = 0.00034437\n",
      "Iteration 19, loss = 0.00031122\n",
      "Iteration 20, loss = 0.00028357\n",
      "Iteration 21, loss = 0.00026283\n",
      "Iteration 22, loss = 0.00024498\n",
      "Iteration 23, loss = 0.00023181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.00021951\n",
      "Iteration 25, loss = 0.00020930\n",
      "Iteration 26, loss = 0.00020046\n",
      "Iteration 27, loss = 0.00019261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61705184\n",
      "Iteration 2, loss = 0.25878670\n",
      "Iteration 3, loss = 0.11305803\n",
      "Iteration 4, loss = 0.06070328\n",
      "Iteration 5, loss = 0.03699266\n",
      "Iteration 6, loss = 0.02597990\n",
      "Iteration 7, loss = 0.01668734\n",
      "Iteration 8, loss = 0.01192193\n",
      "Iteration 9, loss = 0.00886966\n",
      "Iteration 10, loss = 0.00605812\n",
      "Iteration 11, loss = 0.00470480\n",
      "Iteration 12, loss = 0.00317658\n",
      "Iteration 13, loss = 0.00226220\n",
      "Iteration 14, loss = 0.00140157\n",
      "Iteration 15, loss = 0.00101869\n",
      "Iteration 16, loss = 0.00073405\n",
      "Iteration 17, loss = 0.00058718\n",
      "Iteration 18, loss = 0.00049224\n",
      "Iteration 19, loss = 0.00042833\n",
      "Iteration 20, loss = 0.00037772\n",
      "Iteration 21, loss = 0.00033996\n",
      "Iteration 22, loss = 0.00031105\n",
      "Iteration 23, loss = 0.00028615\n",
      "Iteration 24, loss = 0.00026649\n",
      "Iteration 25, loss = 0.00024992\n",
      "Iteration 26, loss = 0.00023615\n",
      "Iteration 27, loss = 0.00022478\n",
      "Iteration 28, loss = 0.00021533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Cross-Validation Accuracy Scores [0.96590909 0.97348485 0.96969697 0.94318182 0.95437262 0.97338403\n",
      " 0.97338403 0.96958175 0.95057034 0.96946565]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf_V2, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9431818181818182, 0.9643031151793089, 0.9734848484848485)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.Series(scores)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clf2 = clf_V2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9499241274658573\n",
      "Testing F1 score: 0.9499303587217579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred_clf2))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred_clf2, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(21.453125, 0.5, 'predicted label')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred_clf2)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels = axis, yticklabels = axis)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[304,  15],\n",
       "       [ 18, 322]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=100,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_sta...\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(100, 100, 100, 50),\n",
       "                                                (100, 100, 50), (100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_gs = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(100,100,100,50),(100,100,50),(100,50),(100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clf_tune = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.952959028831563\n",
      "Testing F1 score: 0.9529648824355909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred_clf_tune))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred_clf_tune, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
